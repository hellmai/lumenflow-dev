id: WU-2203
title: 'Phase 3: Extract withMicroWorktree sync preamble helper without behavior changes'
lane: 'Framework: Core Lifecycle'
type: refactor
status: in_progress
priority: P2
created: 2026-02-26
code_paths:
  - packages/@lumenflow/core/src/micro-worktree.ts
  - packages/@lumenflow/core/src/micro-worktree-shared.ts
tests:
  manual:
    - Execute micro-worktree test subset and confirm pushOnly/skipRemote/baseRef behavior remains
      unchanged
  unit:
    - packages/@lumenflow/core/src/__tests__/micro-worktree.test.ts
  e2e: []
artifacts:
  - .lumenflow/stamps/WU-2203.done
dependencies: []
blocked_by:
  - WU-2202
risks: []
notes: (auto) Add implementation notes, rollout context, or a short summary of the plan/conversation.
requires_review: false
assigned_to: tom@hellm.ai
exposure: backend-only
escalation_triggers: []
requires_human_escalation: false
requires_cso_approval: false
requires_cto_approval: false
requires_design_approval: false
description: 'Context: withMicroWorktree contains inline sync preamble logic that fetches
  origin/main, conditionally fast-forwards local main, and computes baseRef for pushOnly vs standard
  mode. Problem: this logic is harder to reason about and difficult to reuse while preserving exact
  behavior. Solution: extract the preamble into explicit helper(s) that return the derived baseRef
  while preserving pushOnly and skipRemote semantics byte-for-byte.'
acceptance:
  - Extract sync preamble logic from withMicroWorktree into dedicated helper function(s)
  - Preserve current behavior for pushOnly, skipRemote, fetch ordering, and baseRef selection
  - No regression in push retry/rebase and cleanup orchestration
  - micro-worktree test suite assertions for pre-fetch/order/baseRef behavior remain green
claimed_mode: worktree
worktree_path: /home/tom/source/hellmai/os/worktrees/framework-core-lifecycle-wu-2203
claimed_at: 2026-02-26T13:07:23.585Z
baseline_main_sha: b74707e6af022aedea2d31eceba29b2f86d95357
session_id: 5ba2494a-facd-4097-8d33-4c78fe10b8d0
approved_by:
  - tom@hellm.ai
approved_at: 2026-02-26T13:07:23.591Z
